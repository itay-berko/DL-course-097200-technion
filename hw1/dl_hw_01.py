# -*- coding: utf-8 -*-
"""DL_hw_01.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jeDqPh1EU_wg-H4egByKOj2GSaJk09vI
"""

import torch
import torch.nn as nn
import torchvision
import torchvision.datasets as dsets
import torchvision.transforms as transforms

import matplotlib.pyplot as plt
import matplotlib.image as mpimg

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/gdrive')
# %cd /content/gdrive/My\ Drive/Colab\ Notebooks/

def get_mean_and_std(dataset):
    '''Compute the mean and std value of dataset.'''
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)
    mean = torch.zeros(1)
    std = torch.zeros(1)
    print('==> Computing mean and std..')
    
    for inputs, targets in dataloader:
        for i in range(1):
            mean[i] += inputs[:,i,:,:].mean()
            std[i] += inputs[:,i,:,:].std()
            
    mean.div_(len(dataset))
    std.div_(len(dataset))
    
    return mean, std

train_dataset = dsets.FashionMNIST(root='./data', 
                                   train=True, 
                                   transform=transforms.ToTensor(),
                                   download=True)
# train_mean, train_std = get_mean_and_std(train_dataset)
# print(f'train mean: {train_mean}, std: {train_std}')
test_dataset = dsets.FashionMNIST(root='./data', 
                                  train=False, 
                                  transform=transforms.ToTensor())
# test_mean, test_std = get_mean_and_std(test_dataset)
# print(f'train size: {len(train_dataset)}, mean: {test_mean}, std: {test_std}')

print(test_dataset)

# Hyper Parameters 
num_classes = len(train_dataset.classes)
batch_size = 128

# Dataset Loader (Input Pipline)
train_loader = torch.utils.data.DataLoader(dataset=train_dataset, 
                                           batch_size=batch_size, 
                                           shuffle=True)

test_loader = torch.utils.data.DataLoader(dataset=test_dataset, 
                                          batch_size=batch_size, 
                                          shuffle=False)

#"taste" the data
it = iter(train_loader)
im,_ = it.next()
input_size = im[0].shape[1]*im[0].shape[2]
torchvision.utils.save_image(im,'./data/example.png')
print(f'input size={input_size}')

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

plt.imshow(mpimg.imread('./data/example.png'))

print(len(train_dataset))

train_dataset.classes

torch.cuda.is_available()

n_h = 75
n_h2 = 32
drop = 0.4
num_epochs = 150
learning_rate = 0.00005

def get_net():
    return nn.Sequential(
        nn.BatchNorm1d(input_size),
        nn.Linear(input_size, n_h),
        nn.ReLU(),
        nn.BatchNorm1d(n_h),
        nn.Dropout(drop),
        nn.Linear(n_h, n_h2),
        nn.ReLU(),
        nn.BatchNorm1d(n_h2),
        nn.Dropout(drop),
        nn.Linear(n_h2, num_classes),
    )

net = get_net()
if torch.cuda.is_available():
  net = net.cuda()

# Loss and Optimizer
criterion = nn.CrossEntropyLoss()
# optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)
optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)

print(f'Number of parameters: {sum(param.numel() for param in net.parameters())}')
print(f'Num of trainable parameters : {sum(p.numel() for p in net.parameters() if p.requires_grad)}')

# Test the Model
def test_model(curr_model):
  curr_model.eval()
  correct = 0
  total = 0
  epoch_loss = 0
  for images, labels in test_loader:
      images = images.view(-1, 28*28)
      if torch.cuda.is_available():
          images = images.cuda()
          labels = labels.cuda()
      outputs = curr_model(images)
      _, predicted = torch.max(outputs.data, 1)
      
      total += labels.size(0)
      correct += (predicted == labels).sum()
      loss = criterion(outputs, labels)
      epoch_loss += loss * len(labels) / len(test_dataset)
  curr_model_test_acc = float(correct) / float(total)
  return curr_model_test_acc, epoch_loss

# Commented out IPython magic to ensure Python compatibility.
# Training the Model
max_acc = 0
train_acc_vec = []
test_acc_vec = []
train_loss_vec = []
test_loss_vec = []
for epoch in range(num_epochs):
    net.train()
    epoch_acc = 0
    epoch_loss = 0
    for i, (images, labels) in enumerate(train_loader):
        images = images.view(-1, 28*28).to()
        if torch.cuda.is_available():
            images = images.cuda()
            labels = labels.cuda()
        
        # Forward + Backward + Optimize
        optimizer.zero_grad()
        outputs = net(images)
        loss = criterion(outputs, labels)
        acc_all = (outputs.max(dim=1).indices == labels)
        acc = float(acc_all.sum()) / len(labels)
        epoch_acc += acc * len(labels) / len(train_dataset)
        epoch_loss += loss * len(labels) / len(train_dataset)
        loss.backward()
        optimizer.step()

    test_acc, test_loss = test_model(net)
    
    test_acc_vec.append(test_acc)
    train_acc_vec.append(epoch_acc)
    test_loss_vec.append(test_loss)
    train_loss_vec.append(epoch_loss)
    print ('Epoch: [%d/%d], train loss: %.4f, test loss: %.4f, train acc: %.4f, test acc: %4f'
#             %(epoch+1, num_epochs, loss.item(), test_loss, epoch_acc, test_acc))
    if test_acc > max_acc and test_acc > 0.87:
        print('model saved')
        torch.save(net.state_dict(), f'best_model_{n_h}_{n_h2}_{drop}_{num_epochs}.pkl')
        max_acc = test_acc

plt.subplot(2, 1, 1)
plt.title('Accuracy per epoch')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.plot(range(num_epochs), train_acc_vec, 'r', label='Train')
plt.plot(range(num_epochs), test_acc_vec, 'b', label='Test')
plt.legend(loc="lower right")

plt.subplot(2, 1, 2)
plt.title('Loss per epoch')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.plot(range(num_epochs), train_loss_vec, 'r', label='Train')
plt.plot(range(num_epochs), test_loss_vec, 'b', label='Test')
plt.legend(loc="upper right")

plt.subplots_adjust(hspace=1)
plt.show()

plt.subplot(2, 1, 1)
plt.title('Accuracy per epoch')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.plot(range(num_epochs), train_acc_vec, 'r', label='Train')
plt.plot(range(num_epochs), test_acc_vec, 'b', label='Test')
plt.legend(loc="lower right")

plt.subplot(2, 1, 2)
plt.title('Loss per epoch')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.plot(range(num_epochs), train_loss_vec, 'r', label='Train')
plt.plot(range(num_epochs), test_loss_vec, 'b', label='Test')
plt.legend(loc="upper right")

plt.subplots_adjust(hspace=1)
plt.show()

best_model = get_net()
if torch.cuda.is_available():
  best_model = best_model.cuda()
best_model_file = f'best_model_{n_h}_{n_h2}_{drop}_{num_epochs}.pkl'
print(f'loading model {best_model_file}')
best_model.load_state_dict(torch.load(best_model_file))
best_model.eval()
model_test_acc, model_test_loss = test_model(best_model)
print('Accuracy of the model on the 10000 test images: %.4f' % model_test_acc)
print(f'Number of parameters: {sum(param.numel() for param in best_model.parameters())}')
print(f'Num of trainable parameters : {sum(p.numel() for p in best_model.parameters() if p.requires_grad)}')